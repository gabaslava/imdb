{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "base_url = 'https://www.imdb.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping https://www.imdb.com for movie attributes: director, actors, movie critic rating, user rating, movie keywords provided by imdb, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the movie links from my imdb seen movies list\n",
    "\n",
    "section_url = 'https://www.imdb.com/list/ls001933214/?sort=list_order,asc&st_dt=&mode=detail&page=1'\n",
    "#behind section_url there is a list of over 1000 movies I've seen. the list is on my private imdb account\n",
    "\n",
    "def next_section_url(url): #function for extracting section link for the next page\n",
    "    section_url = ''\n",
    "    imdb_seen = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_seen, \"html.parser\")\n",
    "    pagination = soup.find('div', attrs={'class':'list-pagination'})\n",
    "    if pagination.find('a', 'flat-button lister-page-next next-page') != None:\n",
    "        section_url = base_url + pagination.find('a', 'flat-button lister-page-next next-page')['href']\n",
    "    return section_url\n",
    "\n",
    "def movie_links(url): #extracting the links for each movie's imdb page\n",
    "    imdb_seen = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_seen, \"html.parser\")\n",
    "    sub_list = soup.find('div', attrs={'class':'lister list detail sub-list'})\n",
    "    list_links = [base_url + dd.a['href'] for dd in sub_list.findAll('div', attrs={'class':'lister-item mode-detail'})]\n",
    "    return list_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_links [] #appending the movie links list with seen movies links:\n",
    "\n",
    "for i in range(11): #11 is for the eleven pages in my list of movies\n",
    "    movie_links = movie_links + movie_links(section_url)\n",
    "    section_url = next_section_url(section_url) \n",
    "    \n",
    "#saving the list of links\n",
    "import pickle\n",
    "pickling_on = open(\"seen_movies_links.pickle\",\"wb\")\n",
    "pickle.dump(movie_links, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to be scraping the information for each movie from the movie_links list\n",
    "\n",
    "def get_keywords(url): #extracting the keyword list that imdb provides\n",
    "    imdb_kw = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_kw, \"html.parser\")\n",
    "    kw_base = soup.find_all('td', 'soda sodavote')\n",
    "    all_kw = [kw_base[i]['data-item-keyword'] for i in range(len(kw_base))]\n",
    "    return all_kw\n",
    "\n",
    "def get_actors(soup): #extracting either the first 10 actors from the actors list, or less if the movie had less\n",
    "    actor_count = len(soup.find('table', 'cast_list').find_all('tr'))-1\n",
    "    if actor_count<11:\n",
    "        actor_list = [soup.find('table', 'cast_list').find_all('tr')[k+1].text.split('\\n\\n')[2][1:-2] for k in range(actor_count)]\n",
    "    else:\n",
    "        actor_list = [soup.find('table', 'cast_list').find_all('tr')[k+1].text.split('\\n\\n')[2][1:-2] for k in range(10)]\n",
    "    actors = ','.join(actor_list)\n",
    "    return actors\n",
    "\n",
    "\n",
    "def movie_info(movie_url):\n",
    "    imdb_m = get(movie_url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text #with the headers parameter I am trying to avoid movie title translations\n",
    "    soup_m = BeautifulSoup(imdb_m, \"html.parser\")\n",
    "    title_and_year = soup_m.find('title').text[:-7]\n",
    "    user_rating=soup_m.find('div', 'ratingValue').text[1:4]\n",
    "    critic_rating=soup_m.find('div', 'titleReviewBar').contents[1].text[3:5]\n",
    "    director = soup_m.find('div', 'credit_summary_item').text[11:]\n",
    "    actors = get_actors(soup_m)\n",
    "    keywords_link = base_url + soup_m.find('nobr').find('a')['href']\n",
    "    keywords_list = get_keywords(keywords_link)\n",
    "    keywords = ','.join(keywords_list)\n",
    "    joined_info = {'title':title_and_year, 'user_rating':user_rating, 'critic_rating':critic_rating, 'director':director, 'actors':actors, 'keywords':keywords}\n",
    "    return joined_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_attr=[]\n",
    "\n",
    "#scraping movie attributes and appending to the list:\n",
    "for i in range(len(movie_links)):\n",
    "    informacija = movie_info(movie_links[i])\n",
    "    movie_attr.append(informacija)\n",
    "\n",
    "movie_frame = pd.DataFrame.from_dict(movie_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing some cleaning on the gathered dataset:\n",
    "\n",
    "movie_frame['director'] = movie_frame['director'].apply(lambda x: x.strip()) #eliminating the whitespace chars\n",
    "movie_frame['director'] = movie_frame['director'].apply(lambda x: x.split('(')[0]) #leaving only the first and main director of the movie\n",
    "movie_frame['director'] = movie_frame['director'].apply(lambda x: x.split('|')[0]) \n",
    "\n",
    "#converting numeric columns from string to integer/float\n",
    "#also normalizing numbers since for one column max is 10 and another max is 100 but the information type is the same\n",
    "movie_frame['critic_rating'] = pd.to_numeric(movie_frame['critic_rating'], errors='coerce')\n",
    "movie_frame['user_rating'] = pd.to_numeric(movie_frame['user_rating'], errors='coerce')\n",
    "movie_frame['user_rating'] = movie_frame['user_rating'].apply(lambda x: x*10)\n",
    "#adding columns with values of the difference between user given rating and movie critic rating\n",
    "movie_frame['rating_diff_abs'] = 0\n",
    "movie_frame['rating_diff_abs'] = (movie_frame['user_rating']-movie_frame['critic_rating']).abs()\n",
    "#absolute difference\n",
    "movie_frame['rating_diff'] = 0\n",
    "movie_frame['rating_diff'] = movie_frame['user_rating']-movie_frame['critic_rating']\n",
    "\n",
    "#splitting title and and year and creating new column for the year that the movie was released\n",
    "movie_frame['year'] = 'y'\n",
    "movie_frame['year'] = movie_frame['title'].apply(lambda x: x.split('(')[1])\n",
    "movie_frame['year'] = movie_frame['year'].apply(lambda x: x[0:-1])\n",
    "#eliminating the year part from the title\n",
    "movie_frame['title'] = movie_frame['title'].apply(lambda x: x.split('(')[0][:-1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this below is a list of movies that are of preferece in regards to the whole movies list\n",
    "positive_sentiment = pd.read_csv('positive_names.csv', sep='delimiter', engine='python')\n",
    "true_outcome = np.array(positive_sentiment['Title'])\n",
    "\n",
    "#create a new column in the movie data frame for the clasification of the movie. True-if the movie is of preference and False-if it is not\n",
    "movie_frame['outcome'] = 0\n",
    "movie_frame['outcome'] = filmu_freimas['title'].isin(true_outcome)\n",
    "\n",
    "#save the dataframe:\n",
    "movie_frame.to_csv('movie_frame.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping the web for movie reviews from review sites: https://www.rogerebert.com https://3brothersfilm.com http://www.reelviews.net https://www.nytimes.com/reviews/movies https://brightlightsfilm.com http://www.urbancinefile.com.au http://alibi.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for scraping each of the review sites if there's a link to in in the imdb page of the movie\n",
    "def roger_ebert(eberts_f):\n",
    "    reviews=[]\n",
    "    if eberts_f is not None:\n",
    "        links = [base_url + i['href'] for i in eberts_f]  \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('h2', 'page-head') is not None: #in festival reviews this is empty, and i dont need festival reviews\n",
    "                #window.find('article', {'class':'ad'}).decompose()\n",
    "                raw_review = window.find('div', {'itemprop':'reviewBody'}).findAll('p')\n",
    "                clean_review1 = [unicodedata.normalize(\"NFKD\", re.sub('<[^<]+?>', '', i.text.strip())) for i in raw_review] #removing inside tags and \\xa0 and .strip() removes \\r\\n from beginning or end \n",
    "                clean_review2 = [i.replace('\\r\\n', ' ').replace('\\r', ' ').replace('\\\\', '') for i in clean_review1] #some \\r\\n did not vanish - now deleting the manual way\n",
    "                clean_review = [i for i in clean_review2 if not i in 'Advertisement']\n",
    "                reviews.append(' '.join(clean_review))\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def brothers(brothers_f):\n",
    "    reviews = []\n",
    "    if brothers_f is not None:\n",
    "        links = [base_url + i['href'] for i in brothers_f]  \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            raw_review = window.find_all('p')\n",
    "            clean_review = [re.sub('<[^<]+?>', '', i.text) for i in raw_review[:-4]]\n",
    "            reviews.append(' '.join(clean_review))\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def reel_views(reel_f):\n",
    "    reviews = []\n",
    "    if reel_f is not None:\n",
    "        links = [base_url + i['href'] for i in reel_f] \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', {'id':'reelContent'}) is not None:\n",
    "                raw_review = window.find('div', {'id':'reelContent'}).find_all('p')\n",
    "                clean_review1 = [unicodedata.normalize(\"NFKD\", re.sub('<[^<]+?>', '', i.text.strip())) for i in raw_review] #removing \\r\\n\n",
    "                clean_review = [i.replace('\\r\\n', '').replace('\\\\', '') for i in clean_review1] #removing manually\n",
    "                reviews.append(' '.join(clean_review))\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def ny_times(times_f):\n",
    "    reviews = []\n",
    "    if times_f is not None:\n",
    "        links = [base_url + i['href'] for i in times_f] \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            archive = window.find('span', 'kicker-label')\n",
    "            regular_rev = window.find('p', 'css-1pdd3ka etcg8100')\n",
    "            if (archive is not None) and (archive.text == 'Archives'):\n",
    "                raw_review = window.find_all('p', 'story-body-text story-content')\n",
    "                clean_review = [i.text.strip() for i in raw_review[:-1]]\n",
    "                #clean_review = [i.replace('\\\\', '') for i in clean_review1]\n",
    "                reviews.append(' '.join(clean_review))\n",
    "            elif (regular_rev is not None) and (regular_rev.text == 'Movie Review'):\n",
    "                raw_review = window.find('section', {'name':'articleBody'}).find_all('p', 'css-1ygdjhk e2kc3sl0')\n",
    "                clean_review = [i.text.strip() for i in raw_review[:-1]]\n",
    "                #clean_review = [i.replace('\\\\', '') for i in clean_review1]\n",
    "                reviews.append(' '.join(clean_review))      \n",
    "    return reviews\n",
    "\n",
    "                \n",
    "def bright_lights(bl_f):\n",
    "    reviews = []\n",
    "    if bl_f is not None:\n",
    "        links = [base_url + i['href'] for i in bl_f]  \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            raw_review = window.find('div', 'text').text.strip()\n",
    "            clean_review = raw_review.replace('\\r\\n', '').replace('\\n', '')\n",
    "            reviews.append(clean_review)\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def alibi(alibi_f):\n",
    "    reviews = []\n",
    "    if alibi_f is not None:\n",
    "        links = [base_url + i['href'] for i in alibi_f] \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            clean_review = window.find('div', {'itemprop':'reviewBody'}).text\n",
    "            reviews.append(clean_review)\n",
    "    return reviews     \n",
    "    \n",
    "    \n",
    "def cinefile(cinefile_f):\n",
    "    reviews = []\n",
    "    if cinefile_f is not None:\n",
    "        links = [base_url + i['href'] for i in cinefile_f] \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            b_tags = window.find('font', {'class':'articleBody'}).find_all('b') #eliminate b tags\n",
    "            for i in b_tags:\n",
    "                i.decompose()\n",
    "            raw_review = window.find('font', {'class':'articleBody'}).text.strip()\n",
    "            clean_review = raw_review.replace('\\r', '').replace('\\n', '')\n",
    "            reviews.append(clean_review)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for gathering the movie title and all available reviews\n",
    "def the_reviews(url):\n",
    "    imdb_m = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text \n",
    "    soup_m = BeautifulSoup(imdb_m, \"html.parser\")\n",
    "    title = soup_m.find('title').text[:-7] #movie title\n",
    "    metas = soup_m.find('div','titleReviewBarItem titleReviewbarItemBorder').find_all('a')[1]['href']\n",
    "    if metas is not None:\n",
    "        metas_soup = BeautifulSoup(get(url+metas).text, \"html.parser\")\n",
    "        eberts_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['Roger Ebert', 'Rogerebert.com']])\n",
    "        brothers_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['3 Brothers']])\n",
    "        reel_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['ReelViews']])\n",
    "        times_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['New York Times']])\n",
    "        bl_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['Bright Lights Film Journal']])\n",
    "        alibi_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['Alibi.com']])\n",
    "        cinefile_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['Urban Cinefile']])\n",
    "        flick_f = metas_soup.find_all('a', text = [re.compile(p) for p in ['FlickFilosopher.com']])\n",
    "        \n",
    "        lights_reviews = bright_lights(bl_f)\n",
    "        alibi_reviews = alibi(alibi_f)\n",
    "        cinefile_reviews = cinefile(cinefile_f)\n",
    "        ebert_reviews = roger_ebert(eberts_f)\n",
    "        brothers_reviews = brothers(brothers_f)\n",
    "        reel_reviews = reel_views(reel_f)\n",
    "        times_reviews = ny_times(times_f)\n",
    "\n",
    "    info = {'title':title, 'ebert':ebert_reviews, '3brothers':brothers_reviews, 'reel':reel_reviews, 'nytimes':times_reviews, 'bright_lights':lights_reviews, 'alibi':alibi_reviews, 'cinefile':cinefile_reviews}\n",
    "    return info\n",
    "\n",
    "\n",
    "#filling an empty list with movie title and all availabe reviews for that particular movie:\n",
    "review_frame=[]\n",
    "for i in movie_links:\n",
    "    review_frame.append(the_reviews(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since one movie can have multiple critic reviews, below is the part where a list consisting of reviews and outcome solely will be created.\n",
    "this way the 1007 movie sample will be expanded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_and_outcome = []\n",
    "\n",
    "for i in review_frame:\n",
    "    if len(i['3brothers'])>0:\n",
    "        skaicius = len(i['3brothers']) \n",
    "        for k in range(skaicius):\n",
    "                review_and_outcome.append([i['3brothers'][k], i['outcome']])\n",
    "    if len(i['ebert'])>0:\n",
    "        skaicius = len(i['ebert']) \n",
    "        for k in range(skaicius):\n",
    "            review_and_outcome.append([i['ebert'][k], i['outcome']])\n",
    "    if len(i['nytimes'])>0:\n",
    "        skaicius = len(i['nytimes']) \n",
    "        for k in range(skaicius):\n",
    "            review_and_outcome.append([i['nytimes'][k], i['outcome']])\n",
    "    if len(i['reel'])>0:\n",
    "        skaicius = len(i['reel']) \n",
    "        for k in range(skaicius):\n",
    "            review_and_outcome.append([i['reel'][k], i['outcome']])\n",
    "    if len(i['bright_lights'])>0:\n",
    "        skaicius = len(i['bright_lights']) \n",
    "        for k in range(skaicius):\n",
    "            review_and_outcome.append([i['bright_lights'][k], i['outcome']])\n",
    "    if len(i['alibi'])>0:\n",
    "        skaicius = len(i['alibi']) \n",
    "        for k in range(skaicius):\n",
    "            review_and_outcome.append([i['alibi'][k], i['outcome']])\n",
    "    if len(i['cinefile'])>0:\n",
    "        skaicius = len(i['cinefile']) \n",
    "        for k in range(skaicius):\n",
    "            review_and_outcome.append([i['cinefile'][k], i['outcome']])\n",
    "        \n",
    "df = pd.DataFrame(review_and_outcome, columns=['review', 'outcome'])\n",
    "df.to_csv('review_and_outcome.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
