{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "\n",
    "base_url = 'https://www.imdb.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping https://www.imdb.com for movie attributes: director, actors, movie critic rating, user rating, movie keywords provided by imdb,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the movie links from my imdb seen movies list\n",
    "\n",
    "section_url = 'https://www.imdb.com/list/ls001933214/?sort=list_order,asc&st_dt=&mode=detail&page=1'\n",
    "#behind section_url there is a list of over 1000 movies I've seen. the list is on my private imdb account\n",
    "\n",
    "def next_section_url(url): #function for extracting section link for the next page\n",
    "    section_url = ''\n",
    "    imdb_seen = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_seen, \"html.parser\")\n",
    "    pagination = soup.find('div', attrs={'class':'list-pagination'})\n",
    "    if pagination.find('a', 'flat-button lister-page-next next-page') != None:\n",
    "        section_url = base_url + pagination.find('a', 'flat-button lister-page-next next-page')['href']\n",
    "    return section_url\n",
    "\n",
    "def movie_lnks(url): #extracting the links for each movie's imdb page\n",
    "    imdb_seen = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_seen, \"html.parser\")\n",
    "    sub_list = soup.find('div', attrs={'class':'lister list detail sub-list'})\n",
    "    list_links = [base_url + dd.a['href'] for dd in sub_list.findAll('div', attrs={'class':'lister-item mode-detail'})]\n",
    "    return list_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_links = [] #appending the movie links list with seen movies links:\n",
    "\n",
    "for i in range(11): #11 is for the eleven pages in my list of movies\n",
    "    movie_links = movie_links + movie_lnks(section_url)\n",
    "    section_url = next_section_url(section_url) \n",
    "    \n",
    "#saving the list of links\n",
    "import pickle\n",
    "pickling_on = open(\"seen_movies_links.pickle\",\"wb\")\n",
    "pickle.dump(movie_links, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to be scraping the information for each movie from the movie_links list\n",
    "\n",
    "def get_keywords(url): #extracting the keyword list that imdb provides\n",
    "    imdb_kw = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_kw, \"html.parser\")\n",
    "    kw_base = soup.find_all('td', 'soda sodavote')\n",
    "    all_kw = [kw_base[i]['data-item-keyword'] for i in range(len(kw_base))]\n",
    "    return all_kw\n",
    "\n",
    "def get_actors(soup): #extracting either the first 10 actors from the actors list, or less if the movie had less\n",
    "    actor_count = len(soup.find('table', 'cast_list').find_all('tr'))-1\n",
    "    if actor_count<11:\n",
    "        actor_list = [soup.find('table', 'cast_list').find_all('tr')[k+1].text.split('\\n\\n')[2][1:-2] for k in range(actor_count)]\n",
    "    else:\n",
    "        actor_list = [soup.find('table', 'cast_list').find_all('tr')[k+1].text.split('\\n\\n')[2][1:-2] for k in range(10)]\n",
    "    actors = ','.join(actor_list)\n",
    "    return actors\n",
    "\n",
    "\n",
    "def movie_info(movie_url):\n",
    "    imdb_m = get(movie_url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text #with the headers parameter I am trying to avoid movie title translations\n",
    "    soup_m = BeautifulSoup(imdb_m, \"html.parser\")\n",
    "    title_and_year = soup_m.find('title').text[:-7]\n",
    "    user_rating=soup_m.find('div', 'ratingValue').text[1:4]\n",
    "    critic_rating=soup_m.find('div', 'titleReviewBar').contents[1].text[3:5]\n",
    "    director = soup_m.find('div', 'credit_summary_item').text[11:]\n",
    "    actors = get_actors(soup_m)\n",
    "    keywords_link = base_url + soup_m.find('nobr').find('a')['href']\n",
    "    keywords_list = get_keywords(keywords_link)\n",
    "    keywords = ','.join(keywords_list)\n",
    "    joined_info = {'title':title_and_year, 'user_rating':user_rating, 'critic_rating':critic_rating, 'director':director, 'actors':actors, 'keywords':keywords}\n",
    "    return joined_info\n",
    "\n",
    "\n",
    "movie_attr=[]\n",
    "\n",
    "#scraping movie attributes and appending to the list:\n",
    "for i in range(len(movie_links)):\n",
    "    informacija = movie_info(movie_links[i])\n",
    "    movie_attr.append(informacija)\n",
    "\n",
    "movie_frame = pd.DataFrame.from_dict(movie_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this below is a list of movies that are of preferece in regards to the whole movies list\n",
    "positive_sentiment = pd.read_csv('positives(1).csv', sep='delimiter', engine='python', encoding='utf-8')\n",
    "true_outcome = np.array(positive_sentiment['Title'])\n",
    "\n",
    "#create a new column in the movie data frame for the clasification of the movie. True-if the movie is of preference and False-if it is not\n",
    "movie_frame['outcome'] = 0\n",
    "movie_frame['outcome'] = movie_frame['title'].isin(true_outcome)\n",
    "\n",
    "#save the dataframe:\n",
    "movie_frame.to_csv('movie_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Scraping the web for movie reviews from review sites:\n",
    "https://www.rogerebert.com https://3brothersfilm.com http://www.reelviews.net https://www.nytimes.com/reviews/movies https://brightlightsfilm.com http://www.urbancinefile.com.au http://alibi.com https://moviemet.com https://www.popmatters.com/film/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping each of the review sites if there's a link to it in in the imdb page of the movie\n",
    "\n",
    "class ReviewScraping():\n",
    "    def __init__(self):\n",
    "        self.baseurl='https://www.imdb.com'\n",
    "        \n",
    "    def the_reviews(self, movie_url):\n",
    "        imdb_m = get(movie_url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text \n",
    "        soup_m = BeautifulSoup(imdb_m, \"html.parser\")\n",
    "        title = soup_m.find('title').text[:-7] #movie title\n",
    "        rev=soup_m.find('div','titleReviewBarItem titleReviewbarItemBorder').find_all('a')\n",
    "        if len(rev)>1:\n",
    "            critics_revs_soup = BeautifulSoup(get(movie_url+rev[1]['href']).text, \"html.parser\")\n",
    "\n",
    "            ebert_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Roger Ebert', 'Rogerebert.com', 'RogerEbert.com']])\n",
    "            ebert_links = [self.baseurl + i['href'] for i in ebert_f]\n",
    "            brothers_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['3 Brothers']])\n",
    "            brothers_links = [self.baseurl + i['href'] for i in brothers_f]\n",
    "            reel_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['ReelViews']])\n",
    "            reel_links = [self.baseurl + i['href'] for i in reel_f]\n",
    "            times_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['New York Times']])\n",
    "            times_links = [self.baseurl + i['href'] for i in times_f]\n",
    "            bl_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Bright Lights Film Journal']])\n",
    "            bright_lights_links = [self.baseurl + i['href'] for i in bl_f]\n",
    "            alibi_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Alibi.com', 'Alibi', 'alibi.com']])\n",
    "            alibi_links = [self.baseurl + i['href'] for i in alibi_f]\n",
    "            cinefile_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Urban Cinefile']])\n",
    "            cinefile_links = [self.baseurl + i['href'] for i in cinefile_f]\n",
    "            metro = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Movie Metropolis']])\n",
    "            metro_links = [self.baseurl + i['href'] for i in metro]\n",
    "            popmatters = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['PopMatters']])\n",
    "            popmatters_links = [self.baseurl + i['href'] for i in popmatters]\n",
    "\n",
    "            ebert_reviews = self.rogerebert(ebert_links)\n",
    "            lights_reviews = self.bright_lights(bright_lights_links)\n",
    "            alibi_reviews = self.alibi(alibi_links)\n",
    "            cinefile_reviews = self.cinefile(cinefile_links)\n",
    "            brothers_reviews = self.brothers(brothers_links)\n",
    "            reel_reviews = self.reel_views(reel_links)\n",
    "            times_reviews = self.ny_times(times_links)\n",
    "            metro_reviews = self.moviemet(metro_links)\n",
    "            popmatters_reviews = self.popmatt(popmatters_links)\n",
    "            bright_lights_reviews = self.bright_lights(bright_lights_links)\n",
    "            \n",
    "        info = {'title':title, 'ebert':ebert_reviews, '3brothers':brothers_reviews, 'reel':reel_reviews, 'nytimes':times_reviews, 'bright_lights':bright_lights_reviews, 'alibi':alibi_reviews, 'cinefile':cinefile_reviews, 'metropolitan':metro_reviews, 'popmatters':popmatters_reviews}\n",
    "        return info\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def rogerebert(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            if 'rogerebert' in links[i]:\n",
    "                window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "                if window.find('h2', 'page-head') is not None: #this is a structure of a regular review on rogerebert.com / skipping festival movie reviews since those are mixed movies\n",
    "                    raw_review = window.find('div', {'itemprop':'reviewBody'}).findAll('p')\n",
    "                    #uncoding the special characters\n",
    "                    clean_review1 = [unicodedata.normalize(\"NFKD\", i.text.strip()) for i in raw_review] \n",
    "                    #removing whitespace manually since .strip() only works for when those characters ar in the beggining or end of a string\n",
    "                    clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "                    clean_review3 = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "                    clean_review = [i for i in clean_review3 if not i in 'Advertisement']\n",
    "                    reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "        \n",
    "        \n",
    "    def bright_lights(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', 'pf-content') != None:\n",
    "                raw_review = [i.text.strip() for i in window.find('div', 'pf-content') if (i!='\\n' and i.find('p', {'class':\"wp-caption-text\"})==None)]\n",
    "                clean_review1 = ' '.join(raw_review).replace('\\r\\n', '').replace('\\n', '').replace('\\r', '').replace('\\\\', '').replace('\\t','')\n",
    "                clean_review2 = re.sub(r'[^\\x00-\\x7f]',r'', clean_review1)\n",
    "                clean_review = re.sub(r'\\'', r'', clean_review2) \n",
    "                if 'The page you were looking for appears to have been moved, deleted, or it does not exist.' not in clean_review:\n",
    "                    reviews.append(clean_review)\n",
    "        return reviews\n",
    "        \n",
    "\n",
    "    def alibi(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', {'itemprop':'reviewBody'}) is not None:\n",
    "                raw_review = window.find('div', {'itemprop':'reviewBody'}).text.strip()\n",
    "                clean_review1 = raw_review.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '').replace('\\\\', '').replace('\\t','')\n",
    "                clean_review2 = re.sub(r'[^\\x00-\\x7f]',r' ', clean_review1)\n",
    "                clean_review = re.sub(r'\\'', r'', clean_review2)\n",
    "                reviews.append(clean_review)\n",
    "        return reviews  \n",
    "\n",
    "\n",
    "    def cinefile(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            if 'urbancinefile' in links[i]:\n",
    "                window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "                if window.find('font', {'class':'articleBody'}) is not None:\n",
    "                    b_tags = window.find('font', {'class':'articleBody'}).find_all('b') #eliminate b tags\n",
    "                    for i in b_tags:\n",
    "                        i.decompose()\n",
    "                    raw_review = window.find('font', {'class':'articleBody'}).text.strip()\n",
    "                    clean_review1 = raw_review.replace('\\r\\n', ' ').replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', ' ').replace('\\t',' ')\n",
    "                    clean_review2 = re.sub(r'[^\\x00-\\x7f]',r'', clean_review1)\n",
    "                    clean_review = re.sub(r'\\'', r'', clean_review2)\n",
    "                    reviews.append(clean_review)\n",
    "        return reviews\n",
    "\n",
    "\n",
    "    def brothers(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            raw_review = window.findAll('p')\n",
    "            clean_review1 = [re.sub('<[^<]+?>', '', i.text) for i in raw_review[:-4]]\n",
    "            clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "            clean_review = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "            reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "\n",
    "\n",
    "    def reel_views(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', {'id':'reelContent'}) is not None:\n",
    "                raw_review = window.find('div', {'id':'reelContent'}).find_all('p')\n",
    "                clean_review1 = [unicodedata.normalize(\"NFKD\", re.sub('<[^<]+?>', '', i.text.strip())) for i in raw_review] \n",
    "                clean_review2 = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in clean_review1]\n",
    "                clean_review3 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review2]\n",
    "                clean_review = [re.sub(r'\\'', r'', i) for i in clean_review3]\n",
    "                reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "\n",
    "    def moviemet(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            if 'dvdtown' not in links[i]:\n",
    "                window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "                if window.find('div', \"clear\") is not None:\n",
    "                    raw_review = window.find('div', \"clear\").findAll('p')\n",
    "                    clean_review1 = [re.sub('<[^<]+?>', '', i.text) for i in raw_review if i.text[:10]!='Also Read: ']\n",
    "                    clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "                    clean_review3 = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "                    clean_review = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in clean_review3]\n",
    "                    reviews.append(' '.join(clean_review))\n",
    "        return reviews  \n",
    "\n",
    "\n",
    "    def ny_times(self, links):\n",
    "        reviews = []\n",
    "        def review_cleaning(raw_rev):\n",
    "            clean_review1 = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in raw_rev]\n",
    "            clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "            clean_rev = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "            return clean_rev\n",
    "\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            rev_check = window.find('span', \"css-17xtcya\")#cheching if regular or archive\n",
    "            if (rev_check is not None) and rev_check.text == 'Archives':\n",
    "                raw_review = window.find_all('p', 'css-exrw3m evys1bk0')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review]\n",
    "                clean_review = review_cleaning(raw_review2)\n",
    "                reviews.append(' '.join(clean_review))\n",
    "            elif (rev_check is not None) and (rev_check.text == 'Movies' or rev_check.text == 'Arts'):\n",
    "                raw_review = window.find_all('p', 'css-exrw3m evys1bk0')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review[:-2] if (i.find('strong', \"css-8qgvsz ebyp5n10\")==None)]\n",
    "                clean_review = review_cleaning(raw_review2)\n",
    "                reviews.append(' '.join(clean_review))\n",
    "            elif (rev_check is not None) and (rev_check.text == 'Arts'):\n",
    "                raw_review = window.find_all('p', 'css-exrw3m evys1bk0')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review[:-2] ]\n",
    "                clean_review = review_cleaning(raw_review2)\n",
    "                reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "\n",
    "\n",
    "    def popmatt(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', \"body-description\") is not None:\n",
    "                raw_review = window.find('div', \"body-description\").findAll(['p', 'blockquote'])\n",
    "                clean_review1 = [re.sub('<[^<]+?>', '', i.text) for i in raw_review if i.text[:14]!='Splash image: ']\n",
    "                clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "                clean_review3 = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "                clean_review = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in clean_review3]\n",
    "                reviews.append(' '.join(clean_review))\n",
    "        return reviews  \n",
    "\n",
    "scrape = ReviewScraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling an empty list with movie title and all availabe reviews for that particular movie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dictionary=[]\n",
    "for i in movie_links:\n",
    "    review_dictionary.append(scrape.the_reviews(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(review_dataset)\n",
    "#unpivoting the data:\n",
    "df2 = pd.melt(df, id_vars='title', value_vars=['ebert', '3brothers', 'reel', 'nytimes', 'bright_lights', 'alibi', 'cinefile', 'metropolitan', 'popmatters'])\n",
    "#eliminating empty rows:\n",
    "df3 = df2[df2['value'].map(lambda x: ((x!=['']) and x!=\"['']\") and (len(x)>0))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After scraping, some samples have several reviews from the same source. Those need to be separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 movies have more than 1 review. 51 movies have more than 2 review. 17 movies have more than 3 review. 7 movies have more than 4 review. 5 movies have more than 5 review.\n"
     ]
    }
   ],
   "source": [
    "double_idx = []\n",
    "triple_idx = []\n",
    "qudr_idx = []\n",
    "quant_idx = []\n",
    "sixt_idx = []\n",
    "\n",
    "for index, k in df3.iterrows():\n",
    "    if len(k['value'])>1:\n",
    "        double_idx.append(index)\n",
    "    if len(k['value'])>2:\n",
    "        triple_idx.append(index)\n",
    "    if len(k['value'])>3:\n",
    "        qudr_idx.append(index)\n",
    "    if len(k['value'])>4:\n",
    "        quant_idx.append(index)\n",
    "    if len(k['value'])>5:\n",
    "        sixt_idx.append(index)\n",
    "        \n",
    "print(len(double_idx),'movies have more than 1 review.',\\\n",
    "      len(triple_idx),'movies have more than 2 review.',\\\n",
    "      len(qudr_idx),'movies have more than 3 review.',\\\n",
    "      len(quant_idx),'movies have more than 4 review.',\\\n",
    "      len(sixt_idx),'movies have more than 5 review.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new dataframe without the multiple review samples\n",
    "df_upd = df3.drop(double_idx)\n",
    "\n",
    "#separating the multiple rewievs as separate samples and appending to the new dataframe\n",
    "for i,mults in zip(range(6),[double_idx,triple_idx,qudr_idx,quant_idx,sixt_idx]):\n",
    "    multiples = df3.loc[mults]['value'].apply(lambda x:[ x[i]]).to_frame().join(df3[['title', 'variable']])\n",
    "    multiples = multiples[['title', 'variable', 'value']]\n",
    "    df_upd=df_upd.append(multiples, ignore_index=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample now is now to a size of 3514.\n",
    "### Below is a counter of all critic's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size: 3514 \n",
      "\n",
      " ebert            836\n",
      "reel             670\n",
      "nytimes          618\n",
      "cinefile         614\n",
      "popmatters       476\n",
      "alibi            178\n",
      "metropolitan      68\n",
      "3brothers         39\n",
      "bright_lights     15\n",
      "Name: variable, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Total sample size:', len(df_upd),'\\n\\n', df_upd['variable'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding sentiment column to the movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of movies that are of preference in regards to the whole movies list\n",
    "positive_sentiment = pd.read_csv('positives(1).csv', sep='delimiter', engine='python', encoding='utf-8')\n",
    "true_outcome = np.array(positive_sentiment['Title'])\n",
    "\n",
    "#create a new column in the movie data frame for the clasification of the movie. True-if the movie is of preference and False-if it is not\n",
    "df_upd['outcome'] = 0\n",
    "df_upd['outcome'] = df_upd['title'].isin(true_outcome)\n",
    "\n",
    "df_upd.columns = ['title', 'source', 'review', 'outcome']\n",
    "\n",
    "#make reviews as plain strings(not a list)\n",
    "df_upd['review'] = df_upd['review'].apply(lambda x: x[0])\n",
    "\n",
    "\n",
    "#dropping duplicate rows if there appear some\n",
    "df_upd.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the final dataframe\n",
    "df_upd.to_csv('Movie Reviews Final(2 categories).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
