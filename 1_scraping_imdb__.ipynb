{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "base_url = 'https://www.imdb.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping https://www.imdb.com for movie attributes: director, actors, movie critic rating, user rating, movie keywords provided by imdb, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the movie links from my imdb seen movies list\n",
    "\n",
    "section_url = 'https://www.imdb.com/list/ls001933214/?sort=list_order,asc&st_dt=&mode=detail&page=1'\n",
    "#behind section_url there is a list of over 1000 movies I've seen. the list is on my private imdb account\n",
    "\n",
    "def next_section_url(url): #function for extracting section link for the next page\n",
    "    section_url = ''\n",
    "    imdb_seen = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_seen, \"html.parser\")\n",
    "    pagination = soup.find('div', attrs={'class':'list-pagination'})\n",
    "    if pagination.find('a', 'flat-button lister-page-next next-page') != None:\n",
    "        section_url = base_url + pagination.find('a', 'flat-button lister-page-next next-page')['href']\n",
    "    return section_url\n",
    "\n",
    "def movie_links(url): #extracting the links for each movie's imdb page\n",
    "    imdb_seen = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_seen, \"html.parser\")\n",
    "    sub_list = soup.find('div', attrs={'class':'lister list detail sub-list'})\n",
    "    list_links = [base_url + dd.a['href'] for dd in sub_list.findAll('div', attrs={'class':'lister-item mode-detail'})]\n",
    "    return list_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_links [] #appending the movie links list with seen movies links:\n",
    "\n",
    "for i in range(11): #11 is for the eleven pages in my list of movies\n",
    "    movie_links = movie_links + movie_links(section_url)\n",
    "    section_url = next_section_url(section_url) \n",
    "    \n",
    "#saving the list of links\n",
    "import pickle\n",
    "pickling_on = open(\"seen_movies_links.pickle\",\"wb\")\n",
    "pickle.dump(movie_links, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to be scraping the information for each movie from the movie_links list\n",
    "\n",
    "def get_keywords(url): #extracting the keyword list that imdb provides\n",
    "    imdb_kw = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text\n",
    "    soup = BeautifulSoup(imdb_kw, \"html.parser\")\n",
    "    kw_base = soup.find_all('td', 'soda sodavote')\n",
    "    all_kw = [kw_base[i]['data-item-keyword'] for i in range(len(kw_base))]\n",
    "    return all_kw\n",
    "\n",
    "def get_actors(soup): #extracting either the first 10 actors from the actors list, or less if the movie had less\n",
    "    actor_count = len(soup.find('table', 'cast_list').find_all('tr'))-1\n",
    "    if actor_count<11:\n",
    "        actor_list = [soup.find('table', 'cast_list').find_all('tr')[k+1].text.split('\\n\\n')[2][1:-2] for k in range(actor_count)]\n",
    "    else:\n",
    "        actor_list = [soup.find('table', 'cast_list').find_all('tr')[k+1].text.split('\\n\\n')[2][1:-2] for k in range(10)]\n",
    "    actors = ','.join(actor_list)\n",
    "    return actors\n",
    "\n",
    "\n",
    "def movie_info(movie_url):\n",
    "    imdb_m = get(movie_url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text #with the headers parameter I am trying to avoid movie title translations\n",
    "    soup_m = BeautifulSoup(imdb_m, \"html.parser\")\n",
    "    title_and_year = soup_m.find('title').text[:-7]\n",
    "    user_rating=soup_m.find('div', 'ratingValue').text[1:4]\n",
    "    critic_rating=soup_m.find('div', 'titleReviewBar').contents[1].text[3:5]\n",
    "    director = soup_m.find('div', 'credit_summary_item').text[11:]\n",
    "    actors = get_actors(soup_m)\n",
    "    keywords_link = base_url + soup_m.find('nobr').find('a')['href']\n",
    "    keywords_list = get_keywords(keywords_link)\n",
    "    keywords = ','.join(keywords_list)\n",
    "    joined_info = {'title':title_and_year, 'user_rating':user_rating, 'critic_rating':critic_rating, 'director':director, 'actors':actors, 'keywords':keywords}\n",
    "    return joined_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_attr=[]\n",
    "\n",
    "#scraping movie attributes and appending to the list:\n",
    "for i in range(len(movie_links)):\n",
    "    informacija = movie_info(movie_links[i])\n",
    "    movie_attr.append(informacija)\n",
    "\n",
    "movie_frame = pd.DataFrame.from_dict(movie_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing some cleaning on the gathered dataset:\n",
    "\n",
    "movie_frame['director'] = movie_frame['director'].apply(lambda x: x.strip()) #eliminating the whitespace chars\n",
    "movie_frame['director'] = movie_frame['director'].apply(lambda x: x.split('(')[0]) #leaving only the first and main director of the movie\n",
    "movie_frame['director'] = movie_frame['director'].apply(lambda x: x.split('|')[0]) \n",
    "\n",
    "#converting numeric columns from string to integer/float\n",
    "#also normalizing numbers since for one column max is 10 and another max is 100 but the information type is the same\n",
    "movie_frame['critic_rating'] = pd.to_numeric(movie_frame['critic_rating'], errors='coerce')\n",
    "movie_frame['user_rating'] = pd.to_numeric(movie_frame['user_rating'], errors='coerce')\n",
    "movie_frame['user_rating'] = movie_frame['user_rating'].apply(lambda x: x*10)\n",
    "#adding columns with values of the difference between user given rating and movie critic rating\n",
    "movie_frame['rating_diff_abs'] = 0\n",
    "movie_frame['rating_diff_abs'] = (movie_frame['user_rating']-movie_frame['critic_rating']).abs()\n",
    "#absolute difference\n",
    "movie_frame['rating_diff'] = 0\n",
    "movie_frame['rating_diff'] = movie_frame['user_rating']-movie_frame['critic_rating']\n",
    "\n",
    "#splitting title and and year and creating new column for the year that the movie was released\n",
    "movie_frame['year'] = 'y'\n",
    "movie_frame['year'] = movie_frame['title'].apply(lambda x: x.split('(')[1])\n",
    "movie_frame['year'] = movie_frame['year'].apply(lambda x: x[0:-1])\n",
    "#eliminating the year part from the title\n",
    "movie_frame['title'] = movie_frame['title'].apply(lambda x: x.split('(')[0][:-1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this below is a list of movies that are of preferece in regards to the whole movies list\n",
    "positive_sentiment = pd.read_csv('positive_names.csv', sep='delimiter', engine='python')\n",
    "true_outcome = np.array(positive_sentiment['Title'])\n",
    "\n",
    "#create a new column in the movie data frame for the clasification of the movie. True-if the movie is of preference and False-if it is not\n",
    "movie_frame['outcome'] = 0\n",
    "movie_frame['outcome'] = movie_frame['title'].isin(true_outcome)\n",
    "\n",
    "#save the dataframe:\n",
    "movie_frame.to_csv('movie_frame.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the web for movie reviews from review sites: \n",
    "https://www.rogerebert.com \n",
    "https://3brothersfilm.com \n",
    "http://www.reelviews.net https://www.nytimes.com/reviews/movies https://brightlightsfilm.com http://www.urbancinefile.com.au \n",
    "http://alibi.com\n",
    "https://moviemet.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping each of the review sites if there's a link to it in in the imdb page of the movie\n",
    "\n",
    "class ReviewScraping():\n",
    "    def __init__(self):\n",
    "        self.baseurl='https://www.imdb.com'\n",
    "\n",
    "    def the_reviews(self, movie_url):\n",
    "        imdb_m = get(movie_url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).text \n",
    "        soup_m = BeautifulSoup(imdb_m, \"html.parser\")\n",
    "        title = soup_m.find('title').text[:-7] #movie title\n",
    "        rev=soup_m.find('div','titleReviewBarItem titleReviewbarItemBorder').find_all('a')\n",
    "        if len(rev)>1:\n",
    "            critics_revs_soup = BeautifulSoup(get(movie_url+rev[1]['href']).text, \"html.parser\")\n",
    "            \n",
    "            ebert_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Roger Ebert', 'Rogerebert.com', 'RogerEbert.com']])\n",
    "            ebert_links = [self.baseurl + i['href'] for i in ebert_f]\n",
    "            brothers_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['3 Brothers']])\n",
    "            brothers_links = [self.baseurl + i['href'] for i in brothers_f]\n",
    "            reel_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['ReelViews']])\n",
    "            reel_links = [self.baseurl + i['href'] for i in reel_f]\n",
    "            times_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['New York Times']])\n",
    "            times_links = [self.baseurl + i['href'] for i in times_f]\n",
    "            bl_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Bright Lights Film Journal']])\n",
    "            bright_lights_links = [self.baseurl + i['href'] for i in bl_f]\n",
    "            alibi_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Alibi.com', 'Alibi', 'alibi.com']])\n",
    "            alibi_links = [self.baseurl + i['href'] for i in alibi_f]\n",
    "            cinefile_f = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Urban Cinefile']])\n",
    "            cinefile_links = [self.baseurl + i['href'] for i in cinefile_f]\n",
    "            metro = critics_revs_soup.find_all('a', text = [re.compile(p) for p in ['Movie Metropolis']])\n",
    "            metro_links = [self.baseurl + i['href'] for i in metro]\n",
    "            \n",
    "            ebert_reviews = self.rogerebert(ebert_links)\n",
    "            lights_reviews = self.bright_lights(bright_lights_links)\n",
    "            alibi_reviews = self.alibi(alibi_links)\n",
    "            cinefile_reviews = self.cinefile(cinefile_links)\n",
    "            brothers_reviews = self.brothers(brothers_links)\n",
    "            reel_reviews = self.reel_views(reel_links)\n",
    "            times_reviews = self.ny_times(times_links)\n",
    "            metro_reviews = self.moviemet(metro_links)\n",
    "            \n",
    "        info = {'title':title, 'ebert':ebert_reviews, '3brothers':brothers_reviews, 'reel':reel_reviews, 'nytimes':times_reviews, 'bright_lights':lights_reviews, 'alibi':alibi_reviews, 'cinefile':cinefile_reviews, 'metropolitan':metro_reviews}\n",
    "        return info\n",
    "\n",
    "\n",
    "    def rogerebert(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('h2', 'page-head') is not None: #this is a structure of a regular review on rogerebert.com\n",
    "                raw_review = window.find('div', {'itemprop':'reviewBody'}).findAll('p')\n",
    "                #uncoding the special characters\n",
    "                clean_review1 = [unicodedata.normalize(\"NFKD\", i.text.strip()) for i in raw_review] \n",
    "                #removing whitespace manually since .strip() only works for when those characters ar in the beggining or end of a string\n",
    "                clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "                clean_review3 = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "                clean_review = [i for i in clean_review3 if not i in 'Advertisement']\n",
    "                reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "\n",
    "    def bright_lights(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            raw_review = window.find('div', 'text').text.strip()\n",
    "            clean_review1 = raw_review.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '').replace('\\\\', '').replace('\\t','')\n",
    "            clean_review2 = re.sub(r'[^\\x00-\\x7f]',r'', clean_review1)\n",
    "            clean_review = re.sub(r'\\'', r'', clean_review2) \n",
    "            reviews.append(clean_review)\n",
    "        return reviews\n",
    "\n",
    "    \n",
    "    def alibi(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', {'itemprop':'reviewBody'}) is not None:\n",
    "                raw_review = window.find('div', {'itemprop':'reviewBody'}).text.strip()\n",
    "                clean_review1 = raw_review.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '').replace('\\\\', '').replace('\\t','')\n",
    "                clean_review2 = re.sub(r'[^\\x00-\\x7f]',r' ', clean_review1)\n",
    "                clean_review = re.sub(r'\\'', r'', clean_review2)\n",
    "                reviews.append(clean_review)\n",
    "        return reviews     \n",
    "\n",
    "    \n",
    "    def cinefile(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('font', {'class':'articleBody'}) is not None:\n",
    "                b_tags = window.find('font', {'class':'articleBody'}).find_all('b') #eliminate b tags\n",
    "                for i in b_tags:\n",
    "                    i.decompose()\n",
    "                raw_review = window.find('font', {'class':'articleBody'}).text.strip()\n",
    "                clean_review1 = raw_review.replace('\\r\\n', ' ').replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', ' ').replace('\\t',' ')\n",
    "                clean_review2 = re.sub(r'[^\\x00-\\x7f]',r'', clean_review1)\n",
    "                clean_review = re.sub(r'\\'', r'', clean_review2)\n",
    "                reviews.append(clean_review)\n",
    "        return reviews\n",
    "\n",
    "    \n",
    "    def brothers(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            raw_review = window.findAll('p')\n",
    "            clean_review1 = [re.sub('<[^<]+?>', '', i.text) for i in raw_review[:-4]]\n",
    "            clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "            clean_review = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "            reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "    \n",
    "    \n",
    "    def reel_views(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', {'id':'reelContent'}) is not None:\n",
    "                raw_review = window.find('div', {'id':'reelContent'}).find_all('p')\n",
    "                clean_review1 = [unicodedata.normalize(\"NFKD\", re.sub('<[^<]+?>', '', i.text.strip())) for i in raw_review] \n",
    "                clean_review2 = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in clean_review1]\n",
    "                clean_review3 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review2]\n",
    "                clean_review = [re.sub(r'\\'', r'', i) for i in clean_review3]\n",
    "                reviews.append(' '.join(clean_review))\n",
    "        return reviews\n",
    "\n",
    "    \n",
    "    def moviemet(self, links):\n",
    "        reviews = []\n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            if window.find('div', \"clear\") is not None:\n",
    "                raw_review = window.find('div', \"clear\").findAll('p')\n",
    "                clean_review1 = [re.sub('<[^<]+?>', '', i.text) for i in raw_review]\n",
    "                clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "                clean_review3 = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "                clean_review = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in clean_review3]\n",
    "                reviews.append(' '.join(clean_review))\n",
    "        return reviews  \n",
    "    \n",
    "    \n",
    "    def ny_times(self, links):\n",
    "        reviews = []\n",
    "        def review_cleaning(raw_rev):\n",
    "            clean_review1 = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in raw_rev]\n",
    "            clean_review2 = [i.replace('\\r\\n', '').replace('\\r', '').replace('\\n','').replace('\\\\', '').replace('\\t','') for i in clean_review1]\n",
    "            clean_rev = [re.sub(r'\\'', r'', i) for i in clean_review2]\n",
    "            return clean_rev\n",
    "        \n",
    "        for i in range(len(links)):\n",
    "            window = BeautifulSoup(get(links[i]).text, \"html.parser\")\n",
    "            archive = window.find('span', 'kicker-label')\n",
    "            archive2 = window.find('blockquote')\n",
    "            regular_rev1 = window.find('p', 'css-1pdd3ka etcg8100')\n",
    "            regular_rev2 = None\n",
    "            if window.find('h3') is not None:\n",
    "                regular_rev2 = window.find('h3').find('span', 'article-kicker')\n",
    "            regular_rev3 = window.find('span', \"css-17xtcya\")#.find('a')\n",
    "            if (archive is not None) and ((archive.text == 'Archives') or (archive.text == 'Movies')):\n",
    "                raw_review = window.find_all('p', 'story-body-text story-content')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review[:-3]]\n",
    "                clean_review = review_cleaning(raw_review2)\n",
    "                reviews.append(' '.join(clean_review))\n",
    "            elif archive2 is not None:\n",
    "                raw_review = window.find('blockquote').select('blockquote > p')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review]\n",
    "                raw_review3 = [i.replace('&aposs', '').replace('&apos', '') for i in raw_review2]\n",
    "                clean_review = review_cleaning(raw_review3)\n",
    "                reviews.append(' '.join(clean_review))\n",
    "            elif (regular_rev1 is not None) and (regular_rev1.text == 'Movie Review'):\n",
    "                raw_review = window.find('section', {'name':'articleBody'}).find_all('p', 'css-1ygdjhk e2kc3sl0')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review[:-3]]\n",
    "                clean_review = review_cleaning(raw_review2)\n",
    "                reviews.append(' '.join(clean_review)) \n",
    "            elif (regular_rev2 is not None) and (regular_rev2.text == 'Movie Review'):\n",
    "                raw_review = window.find_all('p', 'story-body-text story-content')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review[:-3]]\n",
    "                clean_review = review_cleaning(raw_review2)\n",
    "                reviews.append(' '.join(clean_review))\n",
    "            elif (regular_rev3 is not None) and (regular_rev3.text == 'Movies'):\n",
    "                raw_review = window.find('section', {'name':'articleBody'}).find_all('p', 'css-1ygdjhk evys1bk0')\n",
    "                raw_review2 = [i.text.strip() for i in raw_review]\n",
    "                clean_review = review_cleaning(raw_review2[:-1])\n",
    "                reviews.append(' '.join(clean_review)) \n",
    "        return reviews\n",
    "\n",
    "scrape = ReviewScraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling an empty list with movie title and all availabe reviews for that particular movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dictionary=[]\n",
    "for i in movie_links:\n",
    "    review_dictionary.append(scrape.the_reviews(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(review_dictionary)\n",
    "\n",
    "#unpivoting the dataframe:\n",
    "df2 = pd.melt(df, id_vars='title', value_vars=['3brothers', 'alibi', 'bright_lights', 'cinefile', 'ebert', 'metropolitan', 'nytimes', 'reel'])\n",
    "\n",
    "#removing empty rows:\n",
    "df3 = df2[df2['value'].map(lambda x: (x!='[]') and (x!=\"['']\") and (len(x)> 0))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on scraping, some samples have several reviews from the same source. Those need to be separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exctracting datafreames with double reviews:\n",
    "double_reviews1 = df3.loc[df3['value'].str.contains(\"', '\")]\n",
    "double_reviews2 = df3.loc[df3['value'].str.contains(\"', '\")]\n",
    "#separating the reviews:\n",
    "doubles1 = double_reviews1['value'].apply(lambda x: x.split(\"', '\")[0])\n",
    "doubles2 = double_reviews2['value'].apply(lambda x: x.split(\"', '\")[1])\n",
    "#update the review column:\n",
    "double_reviews1['review']=doubles1\n",
    "double_reviews2['review']=doubles2\n",
    "\n",
    "#drop the rows from the initial dataframe that have double reviews:\n",
    "df3_upd = df3.drop(df3.loc[df3['value'].str.contains(\"', '\")].index)\n",
    "#concatinate the dataframes\n",
    "DF = df3_upd.append(double_reviews1, ignore_index=True).append(double_reviews2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample now is expanded to a size of 2771.\n",
    "### Below is a counter of all critic's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size: 2771 \n",
      "\n",
      " ebert            805\n",
      "nytimes          631\n",
      "cinefile         593\n",
      "reel             445\n",
      "alibi            166\n",
      "metropolitan      62\n",
      "bright_lights     47\n",
      "3brothers         22\n",
      "Name: source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Total sample size:', len(DF),'\\n\\n', DF['variable'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding sentiment column to the movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['outcome'] = 0\n",
    "DF['outcome'] = DF['title'].apply(lambda x: x.split('(')[0][:-1]).isin(true_outcome).astype(int)\n",
    "\n",
    "#rename the columns appropriately:\n",
    "DF.columns = ['title', 'source', 'review', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_csv('title_review_outcome.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
